{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\nfrom transformers import (\n    WhisperTokenizer,\n    WhisperProcessor,\n    WhisperFeatureExtractor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nfrom datasets import Audio\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n \nimport torch\nimport evaluate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = 'openai/whisper-small'\nout_dir = 'whisper_small_atco2'\nepochs = 10\nbatch_size = 32","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\t\natc_dataset_train = load_dataset('jlvdoorn/atco2-asr-atcosim', split='train')\natc_dataset_valid = load_dataset('jlvdoorn/atco2-asr-atcosim', split='validation')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor = WhisperFeatureExtractor.from_pretrained(model_id)\n \ntokenizer = WhisperTokenizer.from_pretrained(model_id, language='English', task='transcribe')\n \nprocessor = WhisperProcessor.from_pretrained(model_id, language='English', task='transcribe')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"atc_dataset_train = atc_dataset_train.cast_column('audio', Audio(sampling_rate=16000))\natc_dataset_valid = atc_dataset_valid.cast_column('audio', Audio(sampling_rate=16000))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(batch):\n    audio = batch['audio']\n \n    batch['input_features'] = feature_extractor(audio['array'], sampling_rate=audio['sampling_rate']).input_features[0]\n \n    batch['labels'] = tokenizer(batch['text']).input_ids\n \n    return batch\n \natc_dataset_train = atc_dataset_train.map(\n    prepare_dataset,\n    num_proc=4\n)\n \natc_dataset_valid = atc_dataset_valid.map(\n    prepare_dataset,\n    num_proc=4\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n \n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{'input_features': feature['input_features']} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt')\n \n        label_features = [{'input_ids': feature['labels']} for feature in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors='pt')\n \n        labels = labels_batch['input_ids'].masked_fill(labels_batch.attention_mask.ne(1), -100)\n \n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n \n        batch['labels'] = labels\n \n        return batch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = WhisperForConditionalGeneration.from_pretrained(model_id)\n \nmodel.generation_config.task = 'transcribe'\n \nmodel.generation_config.forced_decoder_ids = None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric = evaluate.load('wer')\n \ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n \n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n \n    # we do not want to group tokens when computing the metrics\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n \n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n \n    return {'wer': wer}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=out_dir,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    gradient_accumulation_steps=1,\n    learning_rate=0.00001,\n    warmup_steps=1000,\n    bf16=True,\n    fp16=False,\n    num_train_epochs=epochs,\n    evaluation_strategy='epoch',\n    logging_strategy='epoch',\n    save_strategy='epoch',\n    predict_with_generate=True,\n    generation_max_length=225,\n    report_to=['tensorboard'],\n    load_best_model_at_end=True,\n    metric_for_best_model='wer',\n    greater_is_better=False,\n    dataloader_num_workers=8,\n    save_total_limit=2,\n    lr_scheduler_type='constant',\n    seed=42,\n    data_seed=42\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=atc_dataset_train,\n    eval_dataset=atc_dataset_valid,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]}]}